{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FiZm7qth32Xt",
        "62_o6a1s4UxC"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Prep model and environment"
      ],
      "metadata": {
        "id": "kRu42Fm03i1v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dtsmw3Vy2kRz"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install ftfy regex tqdm\n",
        "! pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hd7uLoYj3oQX",
        "outputId": "77c62593-cc8a-476a-bc0e-ca4fb205068b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m645.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.13)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.1.3\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-of0m9ch1\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-of0m9ch1\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.1.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.16.0+cu121)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369497 sha256=16cca2fc95f494a94bc1da887ea3d2d0a5a3c3f0304c3f31bcd40efe69cb6b3a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5ox5snor/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "Successfully built clip\n",
            "Installing collected packages: clip\n",
            "Successfully installed clip-1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import clip\n",
        "import os\n",
        "import glob\n",
        "import pickle\n",
        "import random\n",
        "import concurrent.futures\n",
        "from tqdm import tqdm\n",
        "from pkg_resources import packaging\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "7IyfcC9m6UeR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### lol requirements checking"
      ],
      "metadata": {
        "id": "59NMJG-69q0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze > all_packages.txt"
      ],
      "metadata": {
        "id": "m47cIddn8TOe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "# Your package list\n",
        "my_packages = ['ftfy', 'regex', 'tqdm', 'numpy', 'torch', 'Pillow', 'setuptools']\n",
        "\n",
        "# Function to get installed packages with versions\n",
        "def get_installed_packages():\n",
        "    result = subprocess.run(['pip', 'freeze'], stdout=subprocess.PIPE)\n",
        "    installed_packages = result.stdout.decode('utf-8')\n",
        "    return dict(line.split('==') for line in installed_packages.strip().split('\\n'))\n",
        "\n",
        "# Reading installed packages and their versions\n",
        "installed_packages = get_installed_packages()\n",
        "\n",
        "# Filtering based on your package list\n",
        "filtered_packages = {pkg: ver for pkg, ver in installed_packages.items() if pkg in my_packages}\n",
        "\n",
        "# Read existing requirements.txt and convert to dictionary\n",
        "existing_requirements = {}\n",
        "try:\n",
        "    with open('requirements.txt', 'r') as file:\n",
        "        for line in file:\n",
        "            if '==' in line:\n",
        "                pkg, ver = line.strip().split('==')\n",
        "                existing_requirements[pkg] = ver\n",
        "except FileNotFoundError:\n",
        "    print(\"requirements.txt not found, creating a new one.\")\n",
        "\n",
        "# Merge and update requirements\n",
        "updated_requirements = {**existing_requirements, **filtered_packages}\n",
        "\n",
        "# Writing to requirements.txt\n",
        "with open('requirements.txt', 'w') as req_file:\n",
        "    for pkg, ver in updated_requirements.items():\n",
        "        req_file.write(f'{pkg}=={ver}\\n')\n"
      ],
      "metadata": {
        "id": "rTHZOTYo7yCF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Load model\n",
        "\n"
      ],
      "metadata": {
        "id": "jjcTBkXs-Qk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Torch version:\", torch.__version__)\n",
        "\n",
        "model, preprocess = clip.load(\"ViT-B/32\")\n",
        "model.cuda().eval()\n",
        "input_resolution = model.visual.input_resolution\n",
        "context_length = model.context_length\n",
        "vocab_size = model.vocab_size\n",
        "\n",
        "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
        "print(\"Input resolution:\", input_resolution)\n",
        "print(\"Context length:\", context_length)\n",
        "print(\"Vocab size:\", vocab_size)\n"
      ],
      "metadata": {
        "id": "-UiPTbFx3sQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#clip has different models\n",
        "clip.available_models()"
      ],
      "metadata": {
        "id": "DIw2xoGO3wFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess"
      ],
      "metadata": {
        "id": "q1WaV5MN3xk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Embedding"
      ],
      "metadata": {
        "id": "TuvVoNLd38y5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example Text Tokenize"
      ],
      "metadata": {
        "id": "FiZm7qth32Xt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  clip.tokenize(\"ab\")"
      ],
      "metadata": {
        "id": "r5PasHsq3_2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/MANIFOLD NETS/sowpods.txt', 'r') as file:\n",
        "    words = [line.strip() for line in file]\n",
        "\n",
        "print(words[77764],words[84712])"
      ],
      "metadata": {
        "id": "tSA-Hpgc4BfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize and embed Scrabble Text"
      ],
      "metadata": {
        "id": "62_o6a1s4UxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CLIP model, ensure it's on the same device as your data\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "def tokenize_words(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        # Wrap file lines in tqdm for a progress bar\n",
        "        lines = [line.strip() for line in tqdm(file, desc=\"Tokenizing words\")]\n",
        "    return lines\n",
        "\n",
        "# Replace 'your_file.txt' with the path to your file\n",
        "file_path = '/content/drive/MyDrive/AI DOCKER/sowpods.txt'\n",
        "text_as_tokens = tokenize_words(file_path)"
      ],
      "metadata": {
        "id": "KyNdgDXE4FhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process in batches\n",
        "batch_size = 10000\n",
        "text_features = []\n",
        "\n",
        "for i in tqdm(range(0, len(text_as_tokens), batch_size), desc=\"Processing\"):\n",
        "    batch_texts = text_as_tokens[i:i+batch_size]\n",
        "    text_tokens = clip.tokenize(batch_texts).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        batch_features = model.encode_text(text_tokens).float()\n",
        "        text_features.append(batch_features)\n",
        "\n",
        "# Concatenate all batch features\n",
        "text_features = torch.cat(text_features, dim=0)"
      ],
      "metadata": {
        "id": "A45MG_Ap4S89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving token and embeds as pkl for later use"
      ],
      "metadata": {
        "id": "0c-Kvwmp5bSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path-1 = '/content/drive/MyDrive/MANIFOLD NETS/CLIP/text_as_tokens.pkl'\n",
        "file_path-2 = '/content/drive/MyDrive/MANIFOLD NETS/CLIP/text_features.pkl'\n",
        "\n",
        "with open(file_path-1, 'wb') as file:\n",
        "    pickle.dump(text_as_tokens, file)\n",
        "with open(file_path-2, 'wb') as file:\n",
        "    pickle.dump(text_features, file)"
      ],
      "metadata": {
        "id": "xjWCR1kA4fDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Embedding"
      ],
      "metadata": {
        "id": "DaHuIXWq5mWl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Takes the images stored in gdrive and batches the embedding process, and saves a pickle of the resulting tensor vector and the sorted index with the image folder paths back to drive"
      ],
      "metadata": {
        "id": "cNbwjwN86raR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CLIP model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "# Function to read and preprocess a single image\n",
        "def read_and_preprocess_image(image_file):\n",
        "    try:\n",
        "        image = preprocess(Image.open(image_file)).unsqueeze(0).to(device)\n",
        "        return image\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {image_file}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to batch process images using threading\n",
        "def process_images(folder_paths):\n",
        "    image_files = []\n",
        "    source_info = []  # List to keep track of full image paths and source folders for each image\n",
        "    for folder_path in folder_paths:\n",
        "        folder_image_files = glob.glob(os.path.join(folder_path, '*'))[:5]\n",
        "        image_files.extend(folder_image_files)\n",
        "        source_info.extend([(folder_path, image_file) for image_file in folder_image_files])\n",
        "\n",
        "    images = []\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        for image in executor.map(read_and_preprocess_image, image_files):\n",
        "            if image is not None:\n",
        "                images.append(image)\n",
        "\n",
        "    if images:\n",
        "        images = torch.cat(images, dim=0)\n",
        "        with torch.no_grad():\n",
        "            image_features = model.encode_image(images).float()\n",
        "        return image_features, source_info\n",
        "    else:\n",
        "        return torch.Tensor(), []\n",
        "\n",
        "# Function to save all embeddings to a single .pkl file\n",
        "def save_all_embeddings_to_pkl(embeddings, save_dir, filename):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    with open(os.path.join(save_dir, filename), 'wb') as file:\n",
        "        pickle.dump(embeddings, file)\n",
        "\n",
        "# Path to the main directory and the directory to save embeddings\n",
        "main_dir = \"drive/MyDrive/MANIFOLD NETS/imagenet_sample\"\n",
        "save_dir = \"drive/MyDrive/MANIFOLD NETS/image_embeddings\"\n",
        "\n",
        "# Get all subfolders in the main directory\n",
        "subfolders = [os.path.join(main_dir, f) for f in os.listdir(main_dir) if os.path.isdir(os.path.join(main_dir, f))]\n",
        "subfolder_batches = [subfolders[i:i + 5] for i in range(0, len(subfolders), 5)]\n",
        "\n"
      ],
      "metadata": {
        "id": "kRlWBb1V5p3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding loop"
      ],
      "metadata": {
        "id": "34SykefX56NN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_image_features = []\n",
        "image_source_index = []  # Global index for mapping embeddings to source folders and full image paths\n",
        "\n",
        "for folder_batch in tqdm(subfolder_batches, desc=\"Processing Folders\"):\n",
        "    batch_features, source_info = process_images(folder_batch)\n",
        "    if batch_features.nelement() == 0:\n",
        "        continue\n",
        "    all_image_features.append(batch_features)\n",
        "    image_source_index.extend(source_info)\n",
        "\n",
        "# Save all embeddings and the source index\n",
        "concatenated_features = torch.cat(all_image_features, dim=0)\n",
        "save_all_embeddings_to_pkl(concatenated_features, save_dir, 'all_image_embeddings.pkl')\n",
        "save_all_embeddings_to_pkl(image_source_index, save_dir, 'image_source_index.pkl')"
      ],
      "metadata": {
        "id": "VF7A6hgs5yp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing if index of embeddings matches"
      ],
      "metadata": {
        "id": "oCwTHcC652JG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#index test 2: clip embeddings are not exact!\n",
        "\n",
        "\n",
        "# Load combined embeddings and image source index\n",
        "with open('drive/MyDrive/MANIFOLD NETS/image_embeddings/all_image_embeddings.pkl', 'rb') as f:\n",
        "    combined_embeddings = pickle.load(f)\n",
        "\n",
        "with open('drive/MyDrive/MANIFOLD NETS/image_embeddings/image_source_index.pkl', 'rb') as f:\n",
        "    image_source_index = pickle.load(f)\n",
        "\n",
        "# Function to preprocess and embed an image using CLIP\n",
        "def embed_image(image_path, preprocess, model, device):\n",
        "    image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        image_features = model.encode_image(image).float()\n",
        "    return image_features\n",
        "\n",
        "# Function to compare embeddings with a custom threshold\n",
        "def compare_embeddings(original_embedding, test_embedding, threshold=1e-3):\n",
        "    return torch.isclose(original_embedding, test_embedding, atol=threshold).all().item()\n",
        "\n",
        "# Ensure model is in evaluation mode\n",
        "model.eval()\n",
        "# Indices to test: choose a few random indices\n",
        "num_tests = 5  # Number of tests to perform\n",
        "indices_to_test = random.sample(range(len(combined_embeddings)), num_tests)\n",
        "\n",
        "# Load CLIP model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "# Testing\n",
        "match_threshold = 0.1\n",
        "results = []\n",
        "for index in indices_to_test:\n",
        "    image_path = image_source_index[index][1]  # Get the full image path\n",
        "    test_embedding = embed_image(image_path, preprocess, model, device)\n",
        "    original_embedding = combined_embeddings[index]\n",
        "    match = compare_embeddings(original_embedding, test_embedding, threshold=match_threshold)\n",
        "    results.append((index, image_path, original_embedding.cpu().numpy(), test_embedding.cpu().numpy(), match))\n",
        "\n",
        "# Additional diagnostic information\n",
        "for result in results:\n",
        "    print(f\"Index: {result[0]}, Image Path: {result[1]}\")\n",
        "    print(\"Original Embedding (First 5 values):\", result[2][:5].tolist())  # Direct slicing and conversion\n",
        "    print(\"Test Embedding (First 5 values):\", result[3][0][:5].tolist())     # Direct slicing and conversion\n",
        "    print(\"Match:\", result[4], \"\\n\")\n"
      ],
      "metadata": {
        "id": "FlDQ1bpq50Yx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}